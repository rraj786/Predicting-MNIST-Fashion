{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Fashion Dataset\n",
    "### Predicting the type of clothing using various TensorFlow models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up notebook headers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "Fashion MNIST is a dataset within TensorFlow that contains 70,000 grayscale images of clothing items categorized into 10 classes, such as T-shirts, dresses, and shoes. Each image is 28x28 pixels, making it a popular benchmark for training and testing machine learning models for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Raw Data\n",
    "Read in Fashion MNIST dataset directly via TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Display the shape of the datasets\n",
    "print(\"Fashion MNIST Dataset Dimensions\")\n",
    "print(\"Shape of training images:\", train_images.shape)\n",
    "print(\"Shape of training labels:\", train_labels.shape)\n",
    "print(\"Shape of test images:\", test_images.shape)\n",
    "print(\"Shape of test labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Raw Data\n",
    "Have a peek at some images to see how they are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the first 10 images in the training set\n",
    "plt.figure(figsize = (10, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)  \n",
    "    plt.imshow(train_images[i])\n",
    "    plt.title(f\"Label: {train_labels[i]}\")\n",
    "    plt.axis('off')  \n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Visualising Fashion MNIST Dataset (10 Examples)\", fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are provided as integers, so we will need to map them to the corresponding clothing categories for the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data\n",
    "Let's reshape the data and ensure pixel values in the images are normalised so the models can interpret them correctly. Whilst the labels don't necessarily need to be one-hot encoded, let's do so anyway for completeness. Finally, we'll need to split the entire dataset of 70,000 images into training, validation, and test sets using a 75/15/10 split, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the current train and test sets to create one dataset\n",
    "images = np.concatenate([train_images, test_images], axis = 0)\n",
    "labels = np.concatenate([train_labels, test_labels], axis = 0)\n",
    "\n",
    "# Preprocess the data\n",
    "num_classes = np.max(labels) + 1\n",
    "images_mod = images.reshape((-1, images.shape[1], images.shape[1], 1)).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(images_mod, labels, test_size = 0.25)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of datasets\n",
    "print(\"Fashion MNIST Modified Dataset Dimensions\")\n",
    "print(\"Shape of training images:\", x_train.shape)\n",
    "print(\"Shape of training labels:\", y_train.shape)\n",
    "print(\"Shape of validation images:\", x_val.shape)\n",
    "print(\"Shape of validation labels:\", y_val.shape)\n",
    "print(\"Shape of test images:\", x_test.shape)\n",
    "print(\"Shape of test labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Creation\n",
    "We will be building various TensorFlow models and testing their performance on the test set to decide what the optimal configuration should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple neural network with 2 hidden layers and the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model by adding layers sequentially\n",
    "base = models.Sequential()\n",
    "base.add(layers.Flatten(input_shape = (x_train.shape[1], x_train.shape[1], 1)))\n",
    "base.add(layers.Dense(units = 256, activation = 'relu'))\n",
    "base.add(layers.Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Add output layer\n",
    "base.add(layers.Dense(units = num_classes, activation = 'softmax'))\n",
    "\n",
    "# Compile model\n",
    "base.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# Define a History callback to record training metrics\n",
    "history = History()\n",
    "\n",
    "# Fit model on training data\n",
    "base.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "epochs = np.arange(1, 11)\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "base_preds = base.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(base_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(base_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2: Deep Neural Network (DNN) - No Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a deep neural network with 3 hidden layers and the RMSProp optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model by adding layers sequentially\n",
    "dnn = models.Sequential()\n",
    "dnn.add(layers.Flatten(input_shape = (x_train.shape[1], x_train.shape[1], 1)))\n",
    "\n",
    "# Hidden layers with L2 regularization\n",
    "dnn.add(layers.Dense(256, activation = 'relu'))\n",
    "dnn.add(layers.Dense(128, activation = 'relu'))\n",
    "dnn.add(layers.Dense(64, activation = 'relu'))\n",
    "\n",
    "# Output layer\n",
    "dnn.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "dnn.compile(optimizer = 'rmsprop',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "# Define a History callback to record training metrics\n",
    "history = History()\n",
    "\n",
    "# Fit model on training data\n",
    "dnn.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "dnn_preds = dnn.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(dnn_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(dnn_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3: Deep Neural Network (DNN) - With Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a deep neural network with 3 hidden layers, the RMSProp optimiser, and L2-Regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model by adding layers sequentially\n",
    "dnn_reg = models.Sequential()\n",
    "dnn_reg.add(layers.Flatten(input_shape = (x_train.shape[1], x_train.shape[1], 1)))\n",
    "\n",
    "# Hidden layers with L2 regularization\n",
    "dnn_reg.add(layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "dnn_reg.add(layers.Dense(128, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "dnn_reg.add(layers.Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(0.01)))\n",
    "\n",
    "# Output layer\n",
    "dnn_reg.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "dnn_reg.compile(optimizer = 'rmsprop',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "# Define a History callback to record training metrics\n",
    "history = History()\n",
    "\n",
    "# Fit model on training data\n",
    "dnn_reg.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "dnn_reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "dnn_reg_preds = dnn_reg.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(dnn_reg_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(dnn_reg_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a convolutional neural network with 3 convolutional layers, the SGD optimiser, and a dropout of 0.1 between the last 2 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model by adding layers sequentially\n",
    "cnn = models.Sequential()\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (x_train.shape[1], x_train.shape[1], 1)))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Add convolutional layers for localised feature extraction\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Dropout(0.1))\n",
    "\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dropout(0.1))\n",
    "cnn.add(layers.Dense(64, activation = 'relu'))\n",
    "cnn.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn.compile(optimizer = 'sgd',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "# Define a History callback to record training metrics\n",
    "history = History()\n",
    "\n",
    "# Fit model on training data\n",
    "cnn.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "cnn_preds = cnn.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(cnn_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(cnn_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 5: Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a recurrent neural network with 2 LSTM layers, the Adam optimiser, and L2-Regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model by adding layers sequentially\n",
    "rnn = models.Sequential()\n",
    "rnn.add(layers.Reshape((x_train.shape[1], x_train.shape[1]), input_shape = (x_train.shape[1], x_train.shape[1], 1)))\n",
    "\n",
    "# LSTM layers with L2 regularization\n",
    "rnn.add(layers.LSTM(256, return_sequences = True, kernel_regularizer = regularizers.l2(0.01)))\n",
    "rnn.add(layers.LSTM(128, kernel_regularizer = regularizers.l2(0.01)))\n",
    "\n",
    "# Output layer\n",
    "rnn.add(layers.Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn.compile(optimizer = 'adam',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "# Define a History callback to record training metrics\n",
    "history = History()\n",
    "\n",
    "# Fit model on training data\n",
    "rnn.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "rnn_preds = rnn.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(rnn_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(rnn_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 6: Residual Network (ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a residual neural network with 1 convolutional layer, 2 residual blocks, and the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Residual Block\n",
    "def residual_block(x, filters, kernel_size):\n",
    "    y = layers.Conv2D(filters, kernel_size, padding = 'same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    \n",
    "    y = layers.Conv2D(filters, kernel_size, padding = 'same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    \n",
    "    # Skip connection\n",
    "    if x.shape[-1] != filters:\n",
    "        x = layers.Conv2D(filters, kernel_size = (1, 1), padding = 'same')(x)\n",
    "    y = layers.add([x, y])\n",
    "    y = layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "# Define ResNet model\n",
    "def ResNet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape = input_shape)\n",
    "    \n",
    "    # Initial convolutional layer\n",
    "    x = layers.Conv2D(64, (7, 7), strides = (2, 2), padding = 'same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides = (2, 2), padding = 'same')(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, filters = 64, kernel_size = (3, 3))\n",
    "    x = residual_block(x, filters = 64, kernel_size = (3, 3))\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation = 'softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Create ResNet model\n",
    "input_shape = x_train.shape[1:]\n",
    "resnet = ResNet(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "resnet.compile(optimizer = 'adam',\n",
    "                     loss = 'sparse_categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "resnet.fit(x_train, y_train, batch_size = 32, epochs = 10, callbacks = [history],  validation_data = (x_val, y_val))\n",
    "\n",
    "# Print model summary\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training progress over all epochs.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for training and validation data\n",
    "training_progress(epochs, history.history['accuracy'], history.history['loss'], history.history['val_accuracy'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View performance metrics after predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "resnet_preds = resnet.predict(x_test)\n",
    "\n",
    "# Extract confidence scores for predicted labels\n",
    "confidence_scores = np.max(resnet_preds, axis = 1)\n",
    "\n",
    "# Extract predicted labels\n",
    "y_pred = np.argmax(resnet_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion_matrix_ad(y_test, y_pred, num_classes, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of confidence scores\n",
    "confidence_distribution(confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification report\n",
    "classification_report_ad(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguous Images\n",
    "Using the best model, find the 10 most ambiguous images in the test set based on predicted confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
